# File used to store hyperparameters for various ML models. 
xgb:
  max_depth: 3
  eta: 0.1
  min_child_weight: 1
  subsample: 0.8
  colsample_bytree: 0.9
  scale_pos_weight: 1
  n_estimators: 100
  gamma: .1
  

  ### Params sent by Schreck, slow and less accurate for this dataset
    # objective: "reg:squarederror"
    # n_splits: 1
    # learning_rate: 0.1 
    # n_estimators: 1000
    # max_depth: 10
    # n_jobs: 8
    # colsample_bytree: 0.8995496645826047
    # gamma: 0.6148001693726943
    # learning_rate: 0.07773680788294579
    # max_depth: 10 
    # subsample: 0.7898672617361431
    # metric: "valid_rmse"


rf:
  n_estimators: 25 # Number of trees in the forest
  criterion: "squared_error" # Function to measure the quality of a split (previously "mse")
  max_depth: 5 # Maximum depth of the tree
  min_samples_split: 2 # Minimum number of samples required to split an internal node
  min_samples_leaf: 1 # Minimum number of samples required to be at a leaf node
  max_features: .8 # Number of features to consider when looking for the best split
  bootstrap: true # Whether bootstrap samples are used when building trees
  max_samples: null # If bootstrap is True, the number of samples to draw from X to train each base estimator
  random_state: null # Controls both the randomness of the bootstrapping of the samples and the sampling of the features
  verbose: 0 # Controls the verbosity when fitting and predicting
  warm_start: false # When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble

mlp:
    hidden_units: 10
    activation: relu
    optimizer: adam
    epochs: 10
    batch_size: 32
    validation_split: 0.2
    dropout: 0.2
    learning_rate: 0.001

lm:
  fit_intercept: true
  

  