{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6fb94d-e324-436c-90da-64868202cdd7",
   "metadata": {},
   "source": [
    "# Model Tutorial: XGBoost\n",
    "\n",
    "The purpose of this notebook is to demonstrate how to train and predict XGBoost models used in this project. First, we will demonstrate the basic code, and then reproduce the results using a custom class `XGB` to make the code consistent for multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acea576-0a24-40b0-abee-fb0e189eee3d",
   "metadata": {},
   "source": [
    "## Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c1740b-5ee6-4699-b0ef-05f8a4f8a073",
   "metadata": {},
   "source": [
    "The goal is to forecast fuel moisture based on atmospheric data observations using machine learning models. The inputs include equilibrium moisture, calculated from relative humidity and surface temperature, collected from RAWS ground-based stations.\n",
    "\n",
    "XGBoost is a variety of machine learning model that maps an input matrix of features to an output vector. This method can model regression problems, where the target output vector is a continuous quantity. Rows of the feature matrix are observed meteorological quantities at a certain location and time. Values of the output vector are observed fuel moisture quantities at corresponding locations and times.\n",
    "\n",
    "XGBoost is a form of gradient boosting, which uses an ensemble of \"weak learners\" to model the target input. \n",
    "\n",
    "The final model outputs are time series of fuel moisture predictions. The model accuracy is calculated by comparing predicted fuel moisture to observed fuel moisture *at future times* and *at unobserved locations*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cd9b43-5505-4e1d-9647-79c24311fc6a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba1df9-254c-4f34-a858-d9ab817c0202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xg\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yaml\n",
    "# Local modules\n",
    "from fmda_models import XGB\n",
    "import reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf873ab7-36cb-49f9-9752-0d9a0a3c3fca",
   "metadata": {},
   "source": [
    "## Read and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b89336e-82cc-4a7a-86b9-68355c50e107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/rocky_2023_05-09.pkl\")\n",
    "# Remove NA fm\n",
    "df = df.dropna(subset=['fm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0232537-ebf0-44c2-b175-22bc08078e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "reproducibility.set_seed(123)\n",
    "\n",
    "# Create Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[[\"Ed\", \"Ew\"]], df['fm'], test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826d22d-be79-4220-91ab-ff93173fb3db",
   "metadata": {},
   "source": [
    "## Manually Code XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47875768-5df1-454e-beb0-640b6c615397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create model instance\n",
    "bst = xg.XGBRegressor(max_depth=3, eta=.1, min_child_weight=1, subsample=0.8, colsample_bytree=0.8,scale_pos_weight=1,\n",
    "                      objective='reg:squarederror')\n",
    "# fit model\n",
    "bst.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f411dd99-5bfe-4bc4-b265-a01211b0edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted = bst.predict(X_train)\n",
    "preds = bst.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c33735-755e-464c-b9ba-d709ec3bee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE for the training data\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, fitted))\n",
    "\n",
    "# Calculate R-squared for the training data\n",
    "r2_train = r2_score(y_train, fitted)\n",
    "\n",
    "# Calculate RMSE for the test data\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\n",
    "# Calculate R-squared for the test data\n",
    "r2_test = r2_score(y_test, preds)\n",
    "\n",
    "print(\"RMSE for training data:\", rmse_train)\n",
    "print(\"R-squared for training data:\", r2_train)\n",
    "print(\"RMSE for test data:\", rmse_test)\n",
    "print(\"R-squared for test data:\", r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6568c4f-6312-4acf-a50c-bf533adff7cd",
   "metadata": {},
   "source": [
    "## Reproduce using XGB Class\n",
    "\n",
    "We now use a class `XGB` that reproduces the code above. The purpose of the class is to have different machine learning models with the same methods for concise code.\n",
    "\n",
    "The `XGB` class accepts a dictionary for hyperparameters, which can be found in the file `params.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753cba58-b54a-4d1f-9d56-5f656a1d8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('params.yaml', 'r') as file:\n",
    "    all_params = yaml.safe_load(file)\n",
    "\n",
    "params = all_params[\"xgb\"]\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca512ebe-8e5c-4f71-8e93-97b6267ccef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "reproducibility.set_seed(123)\n",
    "\n",
    "model = XGB(loss='reg:squarederror',params=params)\n",
    "model.fit(X_train, y_train)\n",
    "fitted = model.predict(X_train)\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd29618d-0a86-472a-9527-3b64fe7fb46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE for the training data\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, fitted))\n",
    "\n",
    "# Calculate R-squared for the training data\n",
    "r2_train = r2_score(y_train, fitted)\n",
    "\n",
    "# Calculate RMSE for the test data\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\n",
    "# Calculate R-squared for the test data\n",
    "r2_test = r2_score(y_test, preds)\n",
    "\n",
    "print(\"RMSE for training data:\", rmse_train)\n",
    "print(\"R-squared for training data:\", r2_train)\n",
    "print(\"RMSE for test data:\", rmse_test)\n",
    "print(\"R-squared for test data:\", r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e677f068-476f-4928-a328-5a2a5852d10c",
   "metadata": {},
   "source": [
    "## Using Custom Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62442eb6-7a97-4f24-b738-0248ecef39e3",
   "metadata": {},
   "source": [
    "With XGBoost, we need a custom objective function to return the gradient and hessian: [XGBoost Documentation](https://xgboost.readthedocs.io/en/stable/tutorials/custom_metric_obj.html#customized-objective-function). However, `XGBoost` does support weights, so weighted versions of standard loss functions are easily coded.\n",
    "\n",
    "Consider the weighted mean squared error loss function. We define:\n",
    "* $N$: total number of observations\n",
    "* $y^{(i)}_{pred}$: predicted value for $i$th observation from the model\n",
    "* $y^{(i)}_{true}$: observed (\"true\") value for $i$th observation from the model\n",
    "* $w_i$: weight for $i$th observation\n",
    "\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{i=1}^N w_i(y^{(i)}_{pred}-y^{(i)}_{true})^2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ad843-ecc6-4c8d-86fe-797c3b78eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.exp(tf.multiply(-0.01, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e1459f-a0e0-4207-a922-88797bbb21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reproducibility.set_seed(123)\n",
    "# create model instance\n",
    "bst2 = xg.XGBRegressor(max_depth=3, eta=.1, min_child_weight=1, subsample=0.8, colsample_bytree=0.8,scale_pos_weight=1,\n",
    "                      objective='reg:squarederror')\n",
    "# fit model\n",
    "bst2.fit(X_train, y_train, sample_weight = weights)\n",
    "\n",
    "fitted = bst2.predict(X_train)\n",
    "preds = bst2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4411f3b6-1a88-4bc4-beba-f38513f27a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE for the training data\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, fitted))\n",
    "\n",
    "# Calculate R-squared for the training data\n",
    "r2_train = r2_score(y_train, fitted)\n",
    "\n",
    "# Calculate RMSE for the test data\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\n",
    "# Calculate R-squared for the test data\n",
    "r2_test = r2_score(y_test, preds)\n",
    "\n",
    "print(\"RMSE for training data:\", rmse_train)\n",
    "print(\"R-squared for training data:\", r2_train)\n",
    "print(\"RMSE for test data:\", rmse_test)\n",
    "print(\"R-squared for test data:\", r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e789a4e8-ca85-4e15-95c8-df020425a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reproducibility.set_seed(123)\n",
    "model = XGB(loss='reg:squarederror',params=params)\n",
    "model.fit(X_train, y_train, weights)\n",
    "fitted = model.predict(X_train)\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b7ed81-c29b-41c9-957e-d0fcfe82fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE for the training data\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, fitted))\n",
    "\n",
    "# Calculate R-squared for the training data\n",
    "r2_train = r2_score(y_train, fitted)\n",
    "\n",
    "# Calculate RMSE for the test data\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\n",
    "# Calculate R-squared for the test data\n",
    "r2_test = r2_score(y_test, preds)\n",
    "\n",
    "print(\"RMSE for training data:\", rmse_train)\n",
    "print(\"R-squared for training data:\", r2_train)\n",
    "print(\"RMSE for test data:\", rmse_test)\n",
    "print(\"R-squared for test data:\", r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65503fb-2a56-4cba-bee4-4442eeabea20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
