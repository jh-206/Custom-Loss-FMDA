{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25281b97-49a4-43cc-9f88-350a032b3eae",
   "metadata": {},
   "source": [
    "# Autoregression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3931147c-7312-491e-8352-a189a94bc79f",
   "metadata": {},
   "source": [
    "In trying to model an outcome vector based on a feature matrix, autoregression uses one or more outcome values as features to predict future outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a87c28-6082-46d2-9b0f-78051b1ac205",
   "metadata": {},
   "source": [
    "Observed outcome vector:\n",
    "$$\n",
    "y = [y_0, y_1, ..., y_T], \\text{ for }t =0, ..., T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe96db71-9a16-4140-891c-e1094e7be9ea",
   "metadata": {},
   "source": [
    "Lag 1 Model:\n",
    "$$\n",
    "y_t = \\beta_0 + \\beta_1 y_{t-1} + \\epsilon_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat y_t = \\beta_0 + \\beta_1 y_{t-1}\n",
    "$$\n",
    "\n",
    "Usually assuming iid $\\epsilon_t \\sim N(0, \\sigma^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fd269d-40ef-4b86-83da-947a76c3838b",
   "metadata": {},
   "source": [
    "Lag K model:\n",
    "\n",
    "$$\n",
    "\\hat y_t = \\beta_0 + \\sum_{k=1}^K\\beta_k y_{t-k} + \\epsilon_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce299df9-6404-4c23-8f1b-c912249f5e81",
   "metadata": {},
   "source": [
    "Arbitrary Lag model:\n",
    "\n",
    "$K = \\{1, 24\\}$\n",
    "\n",
    "$$\n",
    "\\hat y_t = \\beta_0 + \\sum_{k \\in K}\\beta_k y_{t-k} + \\epsilon_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b419aed-ea29-40d6-828d-da519fd257ff",
   "metadata": {},
   "source": [
    "## Multivariate Linear Autoregression\n",
    "\n",
    "Suppose outcome $y_t$ is a vector of length $m$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f062949-cc1e-4428-9e96-4bdc69097828",
   "metadata": {},
   "source": [
    "$$\n",
    "y_t = \\begin{pmatrix}y_{1t} \\\\ y_{2t}\\\\ \\vdots \\\\ y_{mt} \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c762abf-65ea-4b50-a29c-d2a862949967",
   "metadata": {},
   "source": [
    "$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127aea0c-f5f7-41ac-9cac-0243ec32720c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_0 = \\begin{pmatrix}\\beta^{(0)}_{1} \\\\ \\vdots \\\\ \\beta^{(0)}_{m} \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f5f3ac-9b4c-4821-b469-63e132159c43",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_i = \\begin{pmatrix} \\beta^{(i)}_{11} & ... & \\beta^{(i)}_{1m} \\\\ \n",
    "\\beta^{(i)}_{21} & ... & \\beta^{(i)}_{2m} \\\\\n",
    "... & ... & ... \\\\\n",
    "\\beta^{(i)}_{m1} & ... & \\beta^{(i)}_{mm}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ef2cc-7981-4ebc-9166-e6ee8fae550b",
   "metadata": {},
   "source": [
    "## Multivariate Nonlinear Autoregression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c9bb41-7cc2-466d-940e-7879a05cfa57",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat y_t = f(y_{t-1}, \\ldots, y_{t-K}, \\pmb\\beta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eda6235-7761-48b5-ae05-e51d83215c6a",
   "metadata": {},
   "source": [
    "## Multivariate Nonlinear Autoregression with Additional Inputs\n",
    "\n",
    "Suppose $x_t$ is a vector of feature inputs at time $t$.\n",
    "\n",
    "$$\n",
    "\\hat y_t = f(x_t, y_{t-1}, \\ldots, y_{t-K}, \\pmb\\beta) + \\epsilon_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a5396-4628-4f4e-bc21-b67a109fb708",
   "metadata": {},
   "source": [
    "EXERCISE: \n",
    "\n",
    "Write above in the form below, with $y$ transformed to stacked vector $z$ using: \n",
    "\n",
    "$$z_t=\\begin{pmatrix} y_t \\\\ y_{t-1}\\\\\\vdots \\\\ y_{t-K+1} \\end{pmatrix}$$\n",
    "\n",
    "$$\n",
    "z_t = g(x_t, z_{t-1}, \\pmb\\beta) + \\epsilon_t\n",
    "$$\n",
    "\n",
    "Prove they are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf5977-9805-4f49-8223-428fd47c3b64",
   "metadata": {},
   "source": [
    "*NOTE: * similar motivation to expressing higher-order ODE as system of first-order ODE's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb671cb-730c-42c2-b895-956e0a153179",
   "metadata": {},
   "source": [
    "We have:\n",
    "\n",
    "$$z_{t-1}=\\begin{pmatrix} y_{t-1} \\\\ y_{t-2}\\\\\\vdots \\\\ y_{t-K} \\end{pmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4fc729-d61d-4358-be74-b9c56ce40c0f",
   "metadata": {},
   "source": [
    "### Linear Case\n",
    "\n",
    "$$\n",
    "y_t = \\beta_0 + \\sum_{k=1}^K\\beta_k y_{t-k} + \\epsilon_t\n",
    "$$\n",
    "\n",
    "Where $\\epsilon_t \\sim MVNorm(\\pmb 0, \\Sigma_t)$, where $\\Sigma_t$ is the covariance matrix at time $t$. NOTE: although $\\Sigma_t$ is $m\\times m$, at each time step we get a $m\\times 1$ realization of the random distribution for $\\epsilon_t$.\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}y_{t1}\\\\ \\vdots \\\\ y_{tm}\\end{pmatrix} = \\begin{pmatrix}\\beta^{(0)}_{1} \\\\ \\vdots \\\\ \\beta^{(0)}_{m} \\end{pmatrix} + \\sum_{k=1}^K \\begin{pmatrix} \\beta^{(k)}_{11} & \\ldots & \\beta^{(k)}_{1m}  \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\beta^{(k)}_{m1} & \\ldots & \\beta^{(i)}_{mm}\n",
    "\\end{pmatrix} \\begin{pmatrix}y_{(t-k)1}\\\\ \\vdots \\\\ y_{(t-k)m} \\end{pmatrix} + \\begin{pmatrix} \\epsilon_{t1}\\\\ \\vdots \\\\ \\epsilon_{tm} \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d412d1d9-aca8-4602-9210-2e567cfb1177",
   "metadata": {},
   "source": [
    "$$\n",
    "=\\begin{pmatrix}\\beta^{(0)}_{1} \\\\ \\vdots \\\\ \\beta^{(0)}_{m} \\end{pmatrix} + \\begin{pmatrix} (\\beta_1) & \\ldots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\dots & (\\beta_k) \\end{pmatrix} \\begin{pmatrix} y_{t-1}\\\\ \\vdots \\\\ y_{t-k} \\end{pmatrix} + \\begin{pmatrix} \\epsilon_{t1}\\\\ \\vdots \\\\ \\epsilon_{tm} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Let $z_{t-1} = \\begin{pmatrix} y_{t-1}\\\\ \\vdots \\\\ y_{t-k} \\end{pmatrix}$, so\n",
    "\n",
    "$$\n",
    "y_t = \\beta_0 + \\begin{pmatrix} (\\beta_1) & \\ldots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\dots & (\\beta_k) \\end{pmatrix}z_{t-1} + \\epsilon_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02899eb7-3435-42ae-ae24-1671df55aa09",
   "metadata": {},
   "source": [
    "## Linear Case w External Inputs\n",
    "\n",
    "Ignoring $\\epsilon$ for now...\n",
    "\n",
    "$$\n",
    "y_t = \\beta_0 + \\beta_1 y_{t-1} + \\beta_2 x_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033c789-e50e-4c2c-ac77-6066b8a1b327",
   "metadata": {},
   "source": [
    "And,\n",
    "\n",
    "$$\n",
    "y_{t-1} = \\beta_0 + \\beta_1 y_{t-2} + \\beta_2 x_{t-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e8928-29c8-4b7a-aa56-7e3c89ce5a58",
   "metadata": {},
   "source": [
    "So,\n",
    "\n",
    "$$\n",
    "y_t = \\beta_0 + \\beta_1 [\\beta_0 + \\beta_1 y_{t-2} + \\beta_2 x_{t-1}] + \\beta_2 x_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4babadde-5b30-470d-9654-3a614b216dcc",
   "metadata": {},
   "source": [
    "Thus, with $x_t$ feature input at time $t$ and $y_{t}$ the recurrent state at time $t$ and $d_t$ is observed data at time $t$:\n",
    "\n",
    "$y_t = f(x_t, y_{t-1}, \\beta), \\; L(\\beta)=\\|y_t-d_t\\|^2_2$\n",
    "\n",
    "The function $f$ is understood to be a function of $x$, $y$, and $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba93dc-10a4-4430-ad9c-fe9883886552",
   "metadata": {},
   "source": [
    "Suppose, for clarity, that $y_t\\in\\mathbb R$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{d}{d\\beta} L(\\beta)&=\\frac{d}{d\\beta}(y_t-d_t)^2\\\\\n",
    "    &= 2(y_t-d_t)\\cdot \\frac{d}{d\\beta} y_t \\\\\n",
    "    &= 2(y_t-d_t)\\cdot \\frac{d}{d\\beta} f(x_t, y_{t-1}, \\beta)\\\\\n",
    "    &= 2(y_t-d_t)\\cdot \\left(\\frac{d}{dy}f(x_t, y_{t-1}, \\beta)\\cdot \\frac{d}{d\\beta} y_{t-1}+\\frac{d}{d\\beta}f(x_t, y_{t-1}, \\beta)\\frac{d}{d\\beta}\\beta\\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd3832b-f264-4b26-b84d-54b7828c1978",
   "metadata": {},
   "source": [
    "This requires a recursive relationship:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\beta}y_{t} = \\left(\\frac{d}{dy}f(x_t, y_{t-1}, \\beta)\\cdot \\frac{d}{d\\beta} y_{t-1}+\\frac{d}{d\\beta}f(x_t, y_{t-1}, \\beta)\\frac{d}{d\\beta}\\beta\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d3cc2-2226-4f45-b6a1-f89d5c4e97ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5efd5622-2b55-4597-99f6-b1339d36e3c1",
   "metadata": {},
   "source": [
    "To update parameters in a machine learning context, we need to calculate the gradient of $L(\\beta)$ with respect to $\\beta$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\beta L(\\beta)  = 2 e_t^T\n",
    "$$\n",
    "$$\n",
    "\\nabla_\\beta f(x_t, y_{t-1}, \\beta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bbe467-8443-492f-9e62-5dd2523cb1ac",
   "metadata": {},
   "source": [
    "\n",
    "$y_t = f(x_t, f(x_{t-1}, y_{t-2}, \\beta), \\beta)$\n",
    "\n",
    "$y_t = f(x_t, f(x_{t-1}, f(x_{t-2}, y_{t-3}, \\beta), \\beta), \\beta)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b009f-8234-4c8a-9e35-054214d0c7ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c5b7722-59e5-49b3-89d1-de3cb0eeb95f",
   "metadata": {},
   "source": [
    "## General Case with External Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a02560c-be86-483c-968e-e0cf6ed9a345",
   "metadata": {},
   "source": [
    "For $t=1, ..., T$ let:\n",
    "* $y_t$: modeled state at time $t$\n",
    "* $x_t$: external input at time $t$\n",
    "* $w$: model parameters, constant in time\n",
    "* $d_t$: observed data at time $t$\n",
    "\n",
    "Suppose a simple recursive relationship, where the model at time $t$ is a function of the state at the previous timestep:\n",
    "$$\n",
    "y(t)=f(x_t, y_{t-1}, w)\n",
    "$$\n",
    "\n",
    "Then, suppose we have some loss function $L(y_t, d_t)$. The loss of the model for the entire $T$ timesteps would be $\\frac{1}{T}\\sum_{i=1}^T L(y_t, d_t)$. \n",
    "\n",
    "---\n",
    "\n",
    "*Recall Multivariate Chain Rule:* If $z=f(x,y)$, $x=g(t)$, and $y=h(t)$,\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dt}=\\frac{\\partial f}{\\partial x}\\frac{dx}{dt}+\\frac{\\partial f}{\\partial y}\\frac{dy}{dt}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "To find parameters $w$ that minimize the loss function, we typically calculate the gradients of the loss with respect to the paramters:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial }{\\partial w} \\frac{1}{T}\\sum_{i=1}^T L(y_t, d_t) \n",
    "    &=\\frac{1}{T}\\sum_{i=1}^T \\frac{\\partial L(y_t, d_t)}{\\partial w}  & \\tiny\\text{(since loss is assumed to be differentiable, it is continuous)}\\\\\n",
    "    &=\\frac{1}{T}\\sum_{i=1}^T \\frac{\\partial L(y_t, d_t)}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial w} &\\tiny\\text{(Using chain rule)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The partial derivative of $y_t$ with respect to $w$ is where the recursive relationship is needed. Examining this term more closely, we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial y_t}{\\partial w} &= \\frac{\\partial f(x_t, y_{t-1}, w)}{\\partial w} + \\frac{\\partial f(x_t, y_{t-1}, w)}{\\partial y_{t-1}}\\frac{\\partial y_{t-1}}{\\partial w} + \\frac{\\partial f(x_t, y_{t-1}, w)}{\\partial x_t}\\frac{\\partial x_t}{\\partial w} &\\tiny\\text{(Using chain rule for multivariate functions)}\\\\\n",
    "    &=\\frac{\\partial f(x_t, y_{t-1}, w)}{\\partial w} + \\frac{\\partial f(x_t, y_{t-1}, w)}{\\partial y_{t-1}}\\frac{\\partial y_{t-1}}{\\partial w} &\\tiny\\text{(Since }\\frac{\\partial x_t}{\\partial w} \\tiny\\text{ is zero})\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "We have,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial y_{t-1}}{\\partial w} =\\frac{\\partial f(x_{t-1}, y_{t-2}, w)}{\\partial w} + \\frac{\\partial f(x_{t-1}, y_{t-2}, w)}{\\partial y_{t-2}}\\frac{\\partial y_{t-2}}{\\partial w} \\\\\n",
    "    \\frac{\\partial y_{t-2}}{\\partial w} =\\frac{\\partial f(x_{t-2}, y_{t-3}, w)}{\\partial w} + \\frac{\\partial f(x_{t-2}, y_{t-3}, w)}{\\partial y_{t-3}}\\frac{\\partial y_{t-3}}{\\partial w} \n",
    "\\end{align*} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba04fb-dbf5-474b-afb6-98b245af15d4",
   "metadata": {},
   "source": [
    "Subbing this back into before,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial y_t}{\\partial w} &= \\frac{\\partial f(x_t, y_{t-1}, w)}{\\partial w} + \\frac{\\partial f(x_t, y_{t-1}, w)}{\\partial y_{t-1}}\\left[\\frac{\\partial f(x_{t-1}, y_{t-2}, w)}{\\partial w} + \\frac{\\partial f(x_{t-1}, y_{t-2}, w)}{\\partial y_{t-2}}\\frac{\\partial y_{t-2}}{\\partial w} \\right]\\\\\n",
    "    &= \\frac{\\partial f(x_t, y_{t-1}, w)}{\\partial w} + \\frac{\\partial f(x_t, y_{t-1}, w)}{\\partial y_{t-1}}\\left[\\frac{\\partial f(x_{t-1}, y_{t-2}, w)}{\\partial w} + \\frac{\\partial f(x_{t-1}, y_{t-2}, w)}{\\partial y_{t-2}}\\frac{\\partial y_{t-2}}{\\partial w} \\right]\\\\\n",
    "    &=\\frac{\\partial f(x_t, y_{t-1}, w)}{\\partial w} + \\frac{\\partial f(x_t, y_{t-1}, w)}{\\partial y_{t-1}}\\frac{\\partial f(x_{t-1}, y_{t-2}, w)}{\\partial w}+\\frac{\\partial f(x_t, y_{t-1}, w)}{\\partial y_{t-1}}\\frac{\\partial f(x_{t-1}, y_{t-2}, w)}{\\partial y_{t-2}}\\frac{\\partial y_{t-2}}{\\partial w}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80f79b0-b849-47f0-9414-98ae51c2adaa",
   "metadata": {},
   "source": [
    "So we see that $\\frac{\\partial y_t}{\\partial w}$ is a function of $\\frac{\\partial y_{t-1}}{\\partial w}$, which of course would be a function of $\\frac{\\partial y_{t-2}}{\\partial w}$. Thus, a recursive relationship is set up where the partial derivative with respect to the parameters of the model state at time $t$ is a function of the model state at all previous times $t=1, ..., t-1$. If we continued this process, after the first term of the summation (the partial of $f(x_t, y_{t-1}, w)$ with respect to $w$), we have the summation of the partials of $f$ at previous time steps, with the lookback time step increasing by 1 with each iteration until we reach $y_1$. So,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe69a6-3371-4d02-b3c3-ef61db9cb2f7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial y_t}{\\partial w} = \\frac{\\partial f(x_t, y_{t-1}, w)}{\\partial w} + \\sum_{i=1}^T \\left[\\prod_{j=1}^{t}\\frac{\\partial f(x_{t-j+1}, y_{t-j}, w)}{\\partial y_{t-j}}\\right] \\frac{\\partial f(x_{t-i+1}, y_{t-i}, w)}{\\partial w} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607098f6-19c6-4c7e-856b-c906a255b396",
   "metadata": {},
   "source": [
    "The full expression for $\\frac{\\partial y_t}{\\partial w}$ is rarely calculated in practice. This is because,\n",
    "\n",
    "1) The calculation is computationally expensive for large $T$ and a complex $f$\n",
    "2) This can lead to the \"exploding\" or \"vanishing\" gradient problem. If, for example, $T=100$, the product term above would involve multiplying 100 gradient calculations together. If these were all less than $1$, the term would vanish to zero, and if these were all greater than $1$, it would explode to a large number.\n",
    "\n",
    "There are a variety of methods for avoiding this problem, which results in approximating the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36be1f-10f3-48a1-bd76-7cc0b0600f5b",
   "metadata": {},
   "source": [
    "## Gradient Calculation in RNN's\n",
    "\n",
    "Updating parameters in neural networks requires calculation of gradients, which are estimated with the standard backpropogation algorithm. For simple neural networks like the multilayer-perceptron (see models/Presentations/mlp_model_tutorial.ipynb), where the gradient of the loss function with respect to the parameters of the neural network can be calculated starting with the output layer, then passed back through the architecture of the model to compute gradients for hidden layers and then input layers.\n",
    "\n",
    "The recurrent state of a Recurrent Neural Network (RNN) makes gradient estimates more complicated, and the standard methods have to be augmented to perform a *backpropogation through time*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1541ea0d-cdb3-406d-9323-cd742e47ce70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0874e600-74da-4f84-9c91-f7d555649ca3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16d6e43e-e5f0-4981-8858-d2a62c83e4fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09f937aa-4c9f-4116-876f-6d49957d1846",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://d2l.ai/chapter_recurrent-neural-networks/bptt.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d13a659-c4bd-4a9b-a74e-81075b32897d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
